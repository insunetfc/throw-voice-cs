# ğŸš€ ì°¨ì§‘ì‚¬ Intent Classification System

A BERT-based intent classification system for promotional call centers, specifically designed for car insurance dealer outreach.

## ğŸ“‹ Overview

This system classifies dealer responses during promotional calls into 8 intents:
- `fee_question` - ìˆ˜ìˆ˜ë£Œ/í˜œíƒ ë¬¸ì˜
- `about_company` - íšŒì‚¬ ì •ë³´ í™•ì¸  
- `more_questions` - ì„œë¹„ìŠ¤/ì ˆì°¨ ë¬¸ì˜
- `positive` - ê¸ì • ì‘ë‹µ
- `rejection` - ê±°ì ˆ/ë³´ë¥˜
- `other` - ê¸°íƒ€ (ë‹¤ë¥¸ ê±°ë˜ì²˜)
- `fallback` - ì¶”ê°€ ì •ë³´ ìš”ì²­
- `greeting` - ì¸ì‚¬

## ğŸ—ï¸ Project Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ intent_dataset.csv              # Original dataset (cleaned)
â”‚   â”œâ”€â”€ intent_dataset_enhanced.csv     # With synthetic data
â”‚   â””â”€â”€ promotion_call.csv              # Agent call scripts
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ best_model.pth                  # Best trained model
â”‚   â”œâ”€â”€ final_model.pth                 # Final checkpoint
â”‚   â”œâ”€â”€ confusion_matrix.png            # Evaluation metrics
â”‚   â””â”€â”€ conversation_flow_guide.txt     # Usage guide
â”œâ”€â”€ train.py                            # Complete training pipeline
â”œâ”€â”€ inference.py                        # Inference & response system
â”œâ”€â”€ generate_synthetic_data.py          # Data augmentation
â””â”€â”€ README.md                           # This file
```

## ğŸ”§ Requirements

```bash
pip install torch transformers pandas numpy scikit-learn tqdm matplotlib seaborn
```

**Recommended:**
- Python 3.8+
- CUDA-capable GPU (optional but recommended)
- 8GB+ RAM

## ğŸš€ Quick Start

### Step 1: Generate Enhanced Dataset (Optional but Recommended)

```bash
python generate_synthetic_data.py
```

This will:
- Generate 25 synthetic samples per class
- Merge with existing data
- Create `intent_dataset_enhanced.csv`
- Output conversation flow guide

**Expected output:**
```
ğŸ“Š Existing dataset: ~450 samples
ğŸ“Š Synthetic dataset: ~200 samples
âœ… Combined dataset: ~650 samples
```

### Step 2: Train the Model

```bash
python train.py
```

**Training Configuration:**
- Model: `klue/bert-base`
- Epochs: 8 (with early stopping)
- Batch Size: 8
- Learning Rate: 3e-5
- Data Augmentation: Enabled by default

**Expected Training Time:**
- CPU: ~30-45 minutes
- GPU: ~5-10 minutes

**What happens during training:**
1. âœ… Loads and augments data (2-3x original size)
2. âœ… Stratified train/val split (85/15)
3. âœ… Class-weighted sampling for imbalanced data
4. âœ… Training with gradient clipping
5. âœ… Early stopping (patience=3)
6. âœ… Saves best model based on validation accuracy
7. âœ… Generates confusion matrix
8. âœ… Prints classification report

### Step 3: Test the Model

```bash
python inference.py
```

This will:
- Load the best trained model
- Run sample predictions
- Show agent response recommendations

**Example Output:**
```
Dealer: ëª‡ í¼ì„¼íŠ¸ ì£¼ì‹œëŠ” ê±°ì˜ˆìš”?
Intent: fee_question (98.5%)
Agent: ë³´í—˜ë£Œì˜ 7%ë¥¼ ì†Œê°œë£Œë¡œ ìµì¼ ì§€ê¸‰í•´ë“œë¦½ë‹ˆë‹¤.
```

## ğŸ“Š Model Performance

**Expected Results (with augmentation):**

| Metric | Score |
|--------|-------|
| Training Accuracy | ~95-98% |
| Validation Accuracy | ~85-92% |
| Best for Small Dataset | âœ… |

**Class-wise Performance:**
- High accuracy: `fee_question`, `greeting`, `positive`
- Medium accuracy: `about_company`, `more_questions`, `fallback`
- May need more data: `other`, `rejection` (very similar patterns)

## ğŸ¯ Usage in Production

### Basic Inference

```python
from inference import IntentClassifier

# Load model
classifier = IntentClassifier('./model/best_model.pth')

# Single prediction
dealer_text = "ëª‡ í”„ë¡œ ì£¼ì‹œëŠ” ê±´ê°€ìš”?"
intent, confidence = classifier.predict(dealer_text)
print(f"Intent: {intent} ({confidence:.2%})")

# Get suggested agent response
response = classifier.get_response(dealer_text)
print(f"Agent should say: {response['agent_response']}")
```

### Batch Processing

```python
# Process multiple responses
dealer_responses = [
    "ì°¨ì§‘ì‚¬ê°€ ì–´ë””ì˜ˆìš”?",
    "ê´œì°®ë„¤ìš” ëª…í•¨ ì£¼ì„¸ìš”",
    "ì € ë‹¤ë¥¸ ë° í•˜ê³  ìˆì–´ìš”"
]

results = classifier.predict_batch(dealer_responses)
print(results)
```

### Interactive Testing

```python
classifier = IntentClassifier('./model/best_model.pth')
classifier.interactive_test()
```

## ğŸ“ Training Parameters Explained

### Core Settings

```python
# In train.py
USE_AUGMENTATION = True      # Enable data augmentation
AUGMENT_MULTIPLIER = 3       # 3x augmentation (recommended)
EPOCHS = 8                   # Max epochs (early stopping active)
BATCH_SIZE = 8               # Small batch for better gradients
LEARNING_RATE = 3e-5         # Optimal for BERT fine-tuning
MAX_LEN = 64                 # Sufficient for short dealer responses
```

### When to Adjust

**If you have more data (>1000 samples):**
```python
USE_AUGMENTATION = False     # Disable if data is sufficient
BATCH_SIZE = 16              # Larger batch
EPOCHS = 5                   # Fewer epochs needed
```

**If validation accuracy is low (<80%):**
```python
AUGMENT_MULTIPLIER = 5       # More augmentation
EPOCHS = 10                  # More training
LEARNING_RATE = 2e-5         # Lower learning rate
```

**If overfitting (train acc >> val acc):**
```python
AUGMENT_MULTIPLIER = 5       # More diverse data
# Add dropout in model (requires code change)
```

## ğŸ“ˆ Data Augmentation Strategies

The system uses 3 augmentation techniques:

### 1. Synonym Replacement
```
Original: "ëª‡ í”„ë¡œì—ìš”?"
Augmented: "ì–¼ë§ˆ í¼ì„¼íŠ¸ì—ìš”?"
```

### 2. Ending Variation
```
Original: "ê´œì°®ì•„ìš”"
Augmented: "ê´œì°®ë„¤ìš”", "ê´œì°®êµ°ìš”", "ê´œì°®ì£ "
```

### 3. Combined
```
Original: "ëª‡ í”„ë¡œ ì£¼ëŠ”ë°ìš”?"
Augmented: "ì–¼ë§ˆ í¼ì„¼íŠ¸ ì£¼ë‚˜ìš”?"
```

## ğŸ”„ Conversation Flow Integration

The model works best as part of a conversation flow system:

```
1. Agent starts call (uses promotion_call.csv scripts)
2. Dealer responds
3. Model classifies intent
4. System selects appropriate response
5. Agent continues based on intent

Example Flow:
Agent: "ë³´í—˜ë£Œì˜ 7%ë¥¼ ì§€ê¸‰í•´ë“œë¦½ë‹ˆë‹¤"
Dealer: "ëª‡ í”„ë¡œìš”?" [Detected: fee_question]
Agent: "7í”„ë¡œì´ë©° ìµì¼ ì§€ê¸‰ë©ë‹ˆë‹¤" [Clarification]
Dealer: "ì˜¤ ê´œì°®ë„¤ìš”" [Detected: positive]
Agent: "ëª…í•¨ ë³´ë‚´ë“œë¦´ê²Œìš”" [Close with materials]
```

See `model/conversation_flow_guide.txt` for detailed flow logic.

## ğŸ› Troubleshooting

### Issue: Low Validation Accuracy (<70%)

**Solution:**
1. Generate more synthetic data: `AUGMENT_MULTIPLIER = 5`
2. Increase training epochs: `EPOCHS = 12`
3. Check data quality - remove duplicates/mislabeled samples

### Issue: Model predicts same class for everything

**Solution:**
1. Check class imbalance in data
2. Ensure `WeightedRandomSampler` is working
3. Verify label mapping is correct

### Issue: "CUDA out of memory"

**Solution:**
```python
BATCH_SIZE = 4  # Reduce batch size
MAX_LEN = 32    # Reduce sequence length
```

### Issue: Training is too slow (CPU)

**Solution:**
- Use Google Colab (free GPU)
- Reduce augmentation: `AUGMENT_MULTIPLIER = 2`
- Or wait patiently (~30-45 min on CPU)

## ğŸ“ Customization

### Adding New Intent Classes

1. **Update dataset:**
```csv
question,label
ìƒˆë¡œìš´ ì§ˆë¬¸,new_intent_class
```

2. **Update label mapping in train.py:**
```python
label2id = {
    # ... existing labels ...
    "new_intent_class": 8
}
NUM_LABELS = 9  # Update count
```

3. **Add response template in inference.py:**
```python
self.response_templates = {
    # ... existing templates ...
    "new_intent_class": [
        "ìƒˆë¡œìš´ ì‘ë‹µ í…œí”Œë¦¿"
    ]
}
```

4. **Retrain model**

### Using Different Base Model

```python
# In train.py
MODEL_NAME = "klue/roberta-base"  # Instead of klue/bert-base
# or
MODEL_NAME = "monologg/koelectra-base-v3-discriminator"
```

## ğŸ“Š Model Checkpoints

The training saves two checkpoints:

- **`best_model.pth`** - Best validation accuracy (use this)
- **`final_model.pth`** - Final epoch (for comparison)

Each checkpoint contains:
```python
{
    'model_state_dict': ...,
    'optimizer_state_dict': ...,
    'val_acc': float,
    'val_loss': float,
    'label2id': dict,
    'id2label': dict,
    'tokenizer_name': str,
    'max_len': int
}
```

## ğŸ” Best Practices

### Data Quality
- âœ… Remove duplicates before training
- âœ… Fix typos in your dataset
- âœ… Ensure consistent labeling
- âœ… Balance class distribution (use augmentation)

### Training
- âœ… Always use validation set
- âœ… Monitor overfitting (train vs val accuracy)
- âœ… Use early stopping
- âœ… Save best model, not last

### Deployment
- âœ… Test on real dealer responses
- âœ… Log predictions for analysis
- âœ… Retrain periodically with new data
- âœ… Have fallback for low-confidence predictions

## ğŸ“ Integration Example

```python
# Simplified call center integration
class CallCenterAssistant:
    def __init__(self):
        self.classifier = IntentClassifier('./model/best_model.pth')
        self.conversation_history = []
    
    def process_dealer_response(self, dealer_text):
        # Classify intent
        result = self.classifier.get_response(dealer_text)
        
        # Log conversation
        self.conversation_history.append({
            'dealer': dealer_text,
            'intent': result['detected_intent'],
            'confidence': result['confidence'],
            'agent_response': result['agent_response']
        })
        
        # Return suggested response
        return result['agent_response']
    
    def should_end_call(self):
        # End if 2+ consecutive rejections
        if len(self.conversation_history) >= 2:
            last_two = self.conversation_history[-2:]
            if all(h['intent'] == 'rejection' for h in last_two):
                return True
        return False
```

## ğŸ“š Additional Resources

- [KLUE BERT Documentation](https://github.com/KLUE-benchmark/KLUE)
- [Transformers Documentation](https://huggingface.co/docs/transformers)
- [Korean NLP Resources](https://github.com/songys/AwesomeKorean_Data)

## ğŸ¤ Contributing

To improve the model:

1. Add more training samples (especially for low-accuracy classes)
2. Try different augmentation techniques
3. Experiment with model architectures
4. Share your results!

## ğŸ“„ License

This project is for internal use in promotional call centers.

## âš ï¸ Important Notes

- Model accuracy depends heavily on training data quality
- Regular retraining recommended as new patterns emerge
- Always have human oversight for critical decisions
- Low confidence predictions (<70%) should be escalated to supervisors

---

**Questions or Issues?**

Check the conversation flow guide in `model/conversation_flow_guide.txt` for usage patterns!