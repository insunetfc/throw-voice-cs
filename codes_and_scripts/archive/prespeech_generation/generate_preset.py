
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Hybrid preset generator:
- Fillers (ë¬¸ìž/í™•ì¸/ê³µê°/ì‹œê°„ë²Œê¸°í˜•/â€¦): short TTS via /synthesize (single-shot)
- FAQ/chatbot answers: long TTS via /synthesize_stream_start + /synthesize_stream_batch, stitch parts, ensure WAV/8k/mono/Âµ-law

Requires: requests, boto3, python-dotenv, soundfile, ffmpeg in PATH (for safety transcode)
"""

import os, csv, time, argparse, subprocess, shutil, io, struct, hashlib, json
import requests
import boto3
from botocore.exceptions import ClientError
import soundfile as sf
import unicodedata

# ---------- Config ----------
DEFAULT_TTS_URL = os.getenv("TTS_URL", "https://cfd863800ab8.ngrok-free.app/synthesize").rstrip("/")
# Derive base for streaming endpoints if DEFAULT_TTS_URL already ends with /synthesize
TTS_BASE = DEFAULT_TTS_URL[:-len("/synthesize")] if DEFAULT_TTS_URL.endswith("/synthesize") else DEFAULT_TTS_URL
TTS_TOKEN       = os.getenv("TTS_TOKEN", "").strip()
SAMPLE_RATE     = int(os.getenv("FULL_SAMPLE_RATE", "8000"))
AWS_REGION      = os.getenv("AWS_REGION", "ap-northeast-2")
MAX_RETRIES     = 3
RETRY_SLEEP_SEC = 1.0
CHATBOT_URL     = os.getenv("CHATBOT_URL", "http://15.165.60.45:5000/chat")

FILLER_CATEGORIES = ("ë¬¸ìž", "í™•ì¸", "ê³µê°", "ì‹œê°„ë²Œê¸°í˜•")  # short TTS path
MIN_TOKENS_BEFORE_EOS     = 40        # don't allow EOS until at least this many tokens
EOS_GRACE_MS              = 250       # after EOS is seen, keep generating â‰¥ this much audio
SILENCE_HANGOVER_MS       = 350       # require this much *continuous* silence before we stop
RMS_SILENCE_THRESH        = 0.010     # lower = less aggressive silence detection
TRIM_TAIL_DB              = 30        # if you trim silence anywhere, be gentle on the tail

# ---------- DynamoDB ----------
ddb = boto3.resource("dynamodb", region_name=AWS_REGION)
utt_tbl = ddb.Table("UtteranceCache")

# ---------- S3 ----------
s3 = boto3.client("s3", region_name=AWS_REGION)

# ---------- FAQ utterances only (answers come from chatbot) ----------
FAQS = [
    # ìˆ˜ìˆ˜ë£Œ / ë¹„ìš©
    "ìˆ˜ìˆ˜ë£Œê°€ ì–¼ë§ˆì¸ê°€ìš”?",
    "ì™œ 7%ì¸ê°€ìš”?",
    "ë‹¤ë¥¸ ë¹„ìš©ì€ ì—†ë‚˜ìš”?",
    "ì‹¤ì œë¡œ ì œê°€ ë‚´ì•¼ í•˜ëŠ” ê¸ˆì•¡ì€ ì–¼ë§ˆì˜ˆìš”?",
    # ê°€ìž… ì¡°ê±´ / ìžê²©
    "ì‚¬ê³ ê°€ ë§Žì€ë° ê°€ìž…ì´ ê°€ëŠ¥í•©ë‹ˆê¹Œ?",
    "ì–´ë–¤ ì¡°ê±´ì´ ìžˆì–´ì•¼ ê°€ìž…í•  ìˆ˜ ìžˆë‚˜ìš”?",
    "ë‹¤ì´ë ‰íŠ¸ë¡œë§Œ ê°€ëŠ¥í•œê°€ìš”, ì˜¤í”„ë¼ì¸ë„ ë˜ë‚˜ìš”?",
    "ì œê°€ ì§€ê¸ˆ ë³´í—˜ì´ ìžˆëŠ”ë°ë„ ê°€ìž…í•  ìˆ˜ ìžˆë‚˜ìš”?",
    # ì§€ê¸‰ / ì²˜ë¦¬
    "ìˆ˜ìˆ˜ë£ŒëŠ” ì–¸ì œ ì§€ê¸‰ë˜ë‚˜ìš”?",
    "ì²˜ë¦¬ ì‹œê°„ì€ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?",
    "ë°”ë¡œ ê°€ìž…ì´ ë˜ë‚˜ìš”?",
    "ê²¬ì ì€ ì–¸ì œ ë°›ì„ ìˆ˜ ìžˆë‚˜ìš”?",
    # ì‹ ë¢°ë„ / ì„±ê³¼
    "ì²´ê²°ìœ¨ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
    "ë‹¤ë¥¸ ì‚¬ëžŒë“¤ë„ ë§Žì´ ì´ìš©í•˜ë‚˜ìš”?",
    "ì´ë²ˆì— ìƒˆë¡œ ë§Œë“  íŒ€ì€ ì–´ë–¤ íŒ€ì¸ê°€ìš”?",
    "ë¯¿ì„ ë§Œí•œê°€ìš”?",
    # ë‹´ë‹¹ìž / ì—°ë½ì²˜
    "ì•žìœ¼ë¡œ ëˆ„ê°€ ë‹´ë‹¹í•˜ë‚˜ìš”?",
    "ë‹´ë‹¹ìžê°€ ë°”ë€Œë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
    "ì—°ë½ì²˜ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.",
    "ëª…í•¨ì€ ë¬¸ìžë¡œ ë³´ë‚´ì£¼ì‹¤ ìˆ˜ ìžˆë‚˜ìš”?",
    # ì¼ë°˜ ì‘ëŒ€
    "ì§€ê¸ˆ ë°”ë¹ ìš”.",
    "ë‚˜ì¤‘ì— ë‹¤ì‹œ ì „í™” ì£¼ì„¸ìš”.",
    "ë¬¸ìžë¡œ ë³´ë‚´ì£¼ì„¸ìš”.",
    "ê´€ì‹¬ ì—†ì–´ìš”.",
    "ì´ë¯¸ ë‹¤ë¥¸ ë° ê°€ìž…í–ˆì–´ìš”.",
    "ê·¸ë§Œ ì—°ë½í•´ ì£¼ì„¸ìš”."
]

# ---------- Filler categories (short TTS) ----------
CATEGORIES = {
    "ë¬¸ìž": [
        "ìš”ì²­í•˜ì‹  ë‚´ìš©ì„ ë¬¸ìžë¡œ ë°”ë¡œ ë³´ë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
        "ì§€ê¸ˆ ë°”ì˜ì‹œë©´, í•µì‹¬ ë‚´ìš©ë§Œ ë¬¸ìžë¡œ ì•ˆë‚´ë“œë¦´ê²Œìš”.",
        "í•„ìš”í•˜ì‹  ì •ë³´ëŠ” ë¬¸ìžë¡œ ì•ˆë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
    ],
    "test": ["ë„¤, ë‚˜ì¤‘ì— ë‹¤ì‹œ ì—°ë½ë“œë¦¬ê² ìŠµë‹ˆë‹¤."],
    "í™•ì¸": [
        "ë„¤, ê³ ê°ë‹˜ ë§žìœ¼ì‹­ë‹ˆë‹¤.",
        "ë„¤, ë§ì”€í•˜ì‹  ë‚´ìš© í™•ì¸í–ˆìŠµë‹ˆë‹¤.",
        "ë„¤, ê·¸ë ‡ê²Œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.",
        "ì•Œê² ìŠµë‹ˆë‹¤. ê·¸ëŒ€ë¡œ ì²˜ë¦¬í• ê²Œìš”.",
        "í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤, ìž ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.",
        "ë„¤, ì§€ê¸ˆ ë°”ë¡œ ë„ì™€ë“œë¦´ê²Œìš”.",
        "ë„¤, ì•ˆë‚´ë“œë¦° ëŒ€ë¡œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.",
        "ë„¤, ìš”ì²­í•˜ì‹  ë¶€ë¶„ ì ‘ìˆ˜í–ˆìŠµë‹ˆë‹¤.",
        "ë„¤, ë¬¸ì œ ì—†ìŠµë‹ˆë‹¤.",
        "ë„¤, ì´ì–´ì„œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.",
    ],
    "ì„¤ëª…": [
        "ì•ˆë‚´ë“œë¦¬ìžë©´, ì´ëŸ° ì ˆì°¨ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.",
        "ê°„ë‹¨ížˆ ë§ì”€ë“œë¦¬ë©´ìš”â€¦",
        "ë¨¼ì € í•œ ê°€ì§€ í™•ì¸ í›„ ì„¤ëª…ë“œë¦´ê²Œìš”.",
        "ìˆœì„œëŒ€ë¡œ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
        "ìš”ì•½í•´ì„œ ë§ì”€ë“œë¦¬ë©´ìš”â€¦",
        "ìžì„¸ížˆ ì•ˆë‚´ë“œë¦´ê²Œìš”.",
        "ì°¸ê³ ë¡œ, ì´ ë¶€ë¶„ì€ ì´ë ‡ê²Œ ì´í•´í•˜ì‹œë©´ ë©ë‹ˆë‹¤.",
        "ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…ë“œë¦´ê²Œìš”.",
        "í•µì‹¬ë§Œ ì§šì–´ì„œ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
        "ì´ì–´ì„œ ì¶”ê°€ ì„¤ëª… ë“œë¦´ê²Œìš”.",
    ],
    "ê³µê°": [
        "ì•„, ê·¸ëŸ¬ì…¨êµ°ìš”. ë§Žì´ ë¶ˆíŽ¸í•˜ì…¨ê² ì–´ìš”.",
        "ë„¤, ê·¸ ë§ˆìŒ ì´í•´í•©ë‹ˆë‹¤.",
        "ë§ì”€ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.",
        "ì•„, ê·¸ëŸ° ìƒí™©ì´ë©´ ë‹µë‹µí•˜ì…¨ê² ìŠµë‹ˆë‹¤.",
        "ë„¤, ê·¸ë ‡ê²Œ ëŠë¼ì‹¤ ìˆ˜ ìžˆì–´ìš”.",
        "ê³µê°í•©ë‹ˆë‹¤. ë” ì‹ ê²½ ì“°ê² ìŠµë‹ˆë‹¤.",
        "ë¶ˆíŽ¸ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤.",
        "ë„¤, ì¶©ë¶„ížˆ ì´í•´í–ˆìŠµë‹ˆë‹¤.",
        "ì˜ê²¬ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ë°˜ì˜í•´ ë³´ê² ìŠµë‹ˆë‹¤.",
        "ê±±ì •ë˜ì‹¤ ìˆ˜ ìžˆê² ìŠµë‹ˆë‹¤.",
    ],
    "ì‹œê°„ë²Œê¸°í˜•": [
        "ìž ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”, ë°”ë¡œ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.",
        "ì§€ê¸ˆ ì¡°íšŒ ì¤‘ìž…ë‹ˆë‹¤â€¦",
        "ê³§ ê²°ê³¼ ì•ˆë‚´ë“œë¦´ê²Œìš”â€¦",
        "í™•ì¸ê¹Œì§€ 1~2ì´ˆë§Œ ë” ë¶€íƒë“œë¦½ë‹ˆë‹¤â€¦",
        "ìžë£Œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ìž…ë‹ˆë‹¤â€¦",
        "ê¸ˆë°© ì—°ê²°í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤â€¦",
        "ì²˜ë¦¬ ì¤‘ìž…ë‹ˆë‹¤, ìž ì‹œë§Œìš”â€¦",
        "ì´ì–´ì„œ ì¤€ë¹„ ì¤‘ìž…ë‹ˆë‹¤â€¦",
        "ì¡°ê¸ˆë§Œ ë” ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”â€¦",
        "í™•ì¸ì´ ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ ë§ì”€ë“œë¦´ê²Œìš”â€¦",
    ],
#     "test": [
#         "ì•ˆë…•í•˜ì„¸ìš”~ ìžë™ì°¨ ë³´í—˜ ë¹„êµ ê°€ìž… ë„ì™€ë“œë¦¬ëŠ” ì°¨ì§‘ì‚¬ ë‹¤ì´ë ‰íŠ¸ ì°¨ì€í•˜ íŒ€ìž¥ìž…ë‹ˆë‹¤. ìž ì‹œ í†µí™” ê°€ëŠ¥í•˜ì‹¤ê¹Œìš”? ì§€ê¸ˆ ì´ìš©í•˜ê³  ê³„ì‹  ì—…ì²´ ìžˆìœ¼ì‹¤í…ë° ì €í¬ê°€ ì´ë²ˆì— ë³´í—˜ì‚¬ ì—°ë„ëŒ€ìƒìž ì¶œì‹ ë“¤ë¡œ íŒ€ì„ ìž¬êµ¬ì„± í•˜ë©´ì„œ ìˆ˜ìˆ˜ë£Œ 7%í”„ë¡œì˜ ì¡°ê±´ìœ¼ë¡œ ì§„í–‰ì„ í•˜ê³  ìžˆì–´ì„œ ì•ˆë‚´ì°¨ ì—°ë½ë“œë ¸ìŠµë‹ˆë‹¤. ì‚¬ê³ ê±´ì´ ë§Žê±°ë‚˜ í•´ì„œ ë‹¤ì´ë ‰íŠ¸ ê°€ìž…ì´ ì•ˆë˜ì‹œëŠ” ê³ ê°ë‹˜ë“¤ë„ OFF ë¼ì¸ìœ¼ë¡œ ê°€ìž… ê°€ëŠ¥í•˜ê²Œ í•´ë“œë¦¬ê³  OFF, TM, CM ê°€ìž…ì‹œ ëª¨ë‘ 7%ìˆ˜ìˆ˜ë£Œë¥¼ ìµì¼ì˜¤í›„ì— ë°”ë¡œ ì§€ê¸‰ í•´ë“œë¦¬ê³  ìžˆìŠµë‹ˆë‹¤. ìˆ˜ìˆ˜ë£Œ ì¡°ê±´ë„ ì¢‹ì€ë° ì²´ê²°ìœ¨ë„ 95% ì´ìƒì´ë¼ ë§Žì€ ë¶„ë“¤ì´ í•¨ê»˜ í•˜ê³  ê³„ì‹ ë° ì•žìœ¼ë¡œ ë”œëŸ¬ë‹˜(ì‚¬ìž¥ë‹˜) ë‹´ë‹¹ì€ ì œê°€ í• êº¼ë¼ ì¸ì‚¬ì°¨ ì—°ë½ë“œë ¸êµ¬ìš”. ì œ ë²ˆí˜¸ ì €ìž¥í•´ ë‘ì…¨ë‹¤ê°€ ê²¬ì ë¬¸ì˜ ìžˆìœ¼ì‹¤ë•Œ ì—°ë½ì£¼ì‹œë©´ ì €í¬ê°€ ë¹ ë¥´ê²Œ ì§„í–‰ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ëª…í•¨ ë¬¸ìžë¡œ ë‚¨ê²¨ë“œë¦´ê²Œìš”~ ê°ì‚¬í•©ë‹ˆë‹¤.",
#     ],
    
}

# ---------- Neutral Messages for Promotional Calls ----------
NEUTRAL_MESSAGES = {
#     # General neutral responses that work for both positive/negative customer reactions
#     "general": [
#         "ê³ ê°ë‹˜ì˜ ë§ì”€ ìž˜ ë“¤ì—ˆìŠµë‹ˆë‹¤. ê²€í†  í›„ ë‹¤ì‹œ ì—°ë½ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
#         "ë§ì”€í•´ ì£¼ì‹  ë‚´ìš© í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì ì ˆí•œ ì•ˆë‚´ë¥¼ ìœ„í•´ ë‹¤ì‹œ ì—°ë½ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
#         "ê³ ê°ë‹˜ê»˜ ë§žëŠ” ìƒí’ˆ ì•ˆë‚´ë¥¼ ìœ„í•´ ê²€í†  í›„ ì—°ë½ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
#         "ë” ì •í™•í•œ ì •ë³´ ì œê³µì„ ìœ„í•´ í™•ì¸ í›„ ë‹¤ì‹œ ì—°ë½ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
#     ],
    
#     # For busy customers
#     "busy_response": [
#         "ë°”ì˜ì‹  ì¤‘ì— ì£„ì†¡í•©ë‹ˆë‹¤. ê°„ë‹¨ížˆ ë¬¸ìžë¡œ ì•ˆë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
#         "ì‹œê°„ ë‚´ì–´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì •ë³´ë¥¼ ë¬¸ìžë¡œ ë³´ë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
#         "ë°”ì˜ì‹  ê²ƒ ê°™ìœ¼ë‹ˆ í•„ìš”í•œ ì •ë³´ë§Œ ë¬¸ìžë¡œ ì „ë‹¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
#     ],
    
#     # For interested but cautious customers  
#     "consideration": [
#         "ì‹ ì¤‘í•˜ê²Œ ê²€í† í•˜ì‹œëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ìžì„¸í•œ ìžë£Œë¥¼ ë¬¸ìžë¡œ ë³´ë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
#         "ì¶©ë¶„ížˆ ë¹„êµ ê²€í† í•˜ì‹œê¸¸ ë°”ëžë‹ˆë‹¤. ìƒì„¸ ì •ë³´ë¥¼ ë¬¸ìžë¡œ ì•ˆë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
#         "ê³ ë¯¼ë˜ì‹œëŠ” ë¶€ë¶„ì´ ìžˆìœ¼ì‹œêµ°ìš”. ëª…í™•í•œ ì •ë³´ë¥¼ ë¬¸ìžë¡œ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
#     ],
    
#     # For negative responses
#     "not_interested": [
#         "ë§ì”€ ê°ì‚¬í•©ë‹ˆë‹¤. í˜¹ì‹œ ê´€ì‹¬ ìžˆìœ¼ì‹¤ ë•Œë¥¼ ìœ„í•´ ê°„ë‹¨í•œ ì •ë³´ë§Œ ë¬¸ìžë¡œ ë‚¨ê²¨ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
#         "ì´í•´í•©ë‹ˆë‹¤. ì°¸ê³ ìš©ìœ¼ë¡œ ê¸°ë³¸ ì •ë³´ë§Œ ë¬¸ìžë¡œ ë³´ë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
#         "ì•Œê² ìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— í•„ìš”í•˜ì‹¤ ìˆ˜ë„ ìžˆìœ¼ë‹ˆ ì—°ë½ì²˜ë§Œ ë¬¸ìžë¡œ ë‚¨ê²¨ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
#     ]
    
    # "test": [
    #     "(happy) ì˜ê²¬ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ë°˜ì˜í•´ ë³´ê² ìŠµë‹ˆë‹¤. ìž ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”, ë°”ë¡œ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.",
    #     "(friendly) ì˜ê²¬ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ë°˜ì˜í•´ ë³´ê² ìŠµë‹ˆë‹¤. ìž ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”, ë°”ë¡œ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.",
    #     "(sad) ì˜ê²¬ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ë°˜ì˜í•´ ë³´ê² ìŠµë‹ˆë‹¤. ìž ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”, ë°”ë¡œ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.",
    #     # "ë§ì”€í•´ ì£¼ì‹  ë‚´ìš© í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì ì ˆí•œ ì•ˆë‚´ë¥¼ ìœ„í•´ ë‹¤ì‹œ ì—°ë½ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
    #     # "ê³ ê°ë‹˜ê»˜ ë§žëŠ” ìƒí’ˆ ì•ˆë‚´ë¥¼ ìœ„í•´ ê²€í†  í›„ ì—°ë½ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
    #     # "ë” ì •í™•í•œ ì •ë³´ ì œê³µì„ ìœ„í•´ í™•ì¸ í›„ ë‹¤ì‹œ ì—°ë½ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
    # ],
    
    "promotional": [
        "(friendly) ë„¤ ì €í¬ëŠ” ë‹¤ì´ë ‰íŠ¸ ìžë™ì°¨ë³´í—˜ ë¹„êµ ì°¨ì§‘ì‚¬ ì°¨ì€í•˜ íŒ€ìž¥ìž…ë‹ˆë‹¤. ìž ì‹œ í†µí™” ê°€ëŠ¥í•˜ì‹¤ê¹Œìš”? ì§€ê¸ˆ ì´ìš©í•˜ê³  ê³„ì‹  ì—…ì²´ ìžˆìœ¼ì‹¤ í…ë°ìš”, ì €í¬ê°€ ì´ë²ˆì—, ë³´í—˜ì‚¬ ì—°ë„ ëŒ€ìƒìž ì¶œì‹ ë“¤ë¡œ íŒ€ì„ ìž¬êµ¬ì„±í•˜ë©´ì„œ, ìˆ˜ìˆ˜ë£Œ 7%ì˜ ì¡°ê±´ìœ¼ë¡œ ì§„í–‰ì„ í•˜ê³  ìžˆì–´ì„œ, ì•ˆë‚´ì°¨ ì—°ë½ë“œë ¸ìŠµë‹ˆë‹¤. ì‚¬ê³  ê±´ì´ ë§Žê±°ë‚˜ í•´ì„œ, ë‹¤ì´ë ‰íŠ¸ ê°€ìž…ì´ ì•ˆ ë˜ì‹œëŠ” ê³ ê°ë‹˜ë“¤ë„, ì˜¤í”„ë¼ì¸ìœ¼ë¡œ ê°€ìž… ê°€ëŠ¥í•˜ê²Œ í•´ë“œë¦¬ê³  ìžˆìœ¼ë©°, ì˜¤í”„ë¼ì¸, í…”ë ˆë§ˆì¼€íŒ…, ë¹„êµì‚¬ì´íŠ¸ ê°€ìž… ì‹œ ëª¨ë‘, 7% ìˆ˜ìˆ˜ë£Œë¥¼ ìµì¼ ì˜¤í›„ì— ë°”ë¡œ ì§€ê¸‰í•´ë“œë¦¬ê³  ìžˆìŠµë‹ˆë‹¤. ìˆ˜ìˆ˜ë£Œ ì¡°ê±´ë„ ì¢‹ê³ , ì²´ê²°ìœ¨ë„ 95% ì´ìƒì´ë¼, ë§Žì€ ë¶„ë“¤ì´ í•¨ê»˜í•˜ê³  ê³„ì‹ ë°ìš”, ì•žìœ¼ë¡œ ë”œëŸ¬ë‹˜, ì‚¬ìž¥ë‹˜ ë‹´ë‹¹ì€ ì œê°€ í•  ê±°ë¼, ì¸ì‚¬ì°¨ ì—°ë½ë“œë ¸ìŠµë‹ˆë‹¤. ì œ ë²ˆí˜¸ ì €ìž¥í•´ë‘ì…¨ë‹¤ê°€, ê²¬ì  ë¬¸ì˜ ìžˆìœ¼ì‹¤ ë•Œ ì—°ë½ì£¼ì‹œë©´, ì €í¬ê°€ ë¹ ë¥´ê²Œ ì§„í–‰ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ëª…í•¨, ë¬¸ìžë¡œ ë‚¨ê²¨ë“œë¦´ê²Œìš”. ê°ì‚¬í•©ë‹ˆë‹¤.",
    ]
}

def generate_neutral_voices():
    """Generate all neutral voice variations"""
    neutral_rows = []
    
    for category, messages in NEUTRAL_MESSAGES.items():
        print(f"\n--- Generating neutral voices: {category} ---")
        
        for idx, text in enumerate(messages, start=1):
            print(f"Neutral {category} #{idx}: \"{text}\"")
            
            # Prepare text for TTS
            tts_text = prepare_for_tts(text)
            
            # Generate audio using short TTS
            info = synthesize_short(
                TTS_BASE, 
                tts_text, 
                key_prefix=f"neutral/{category}", 
                sr=SAMPLE_RATE, 
                token=TTS_TOKEN
            )
            
            bucket = info.get("bucket")
            src_key = info.get("key")
            
            if not bucket or not src_key:
                print(f"[ERROR] Failed to generate {category} #{idx}")
                continue
            
            # Define final key structure
            dst_key = f"neutral/{category}/{idx:02d}.wav"
            
            # Download and process audio
            raw = fetch_s3_bytes(bucket, src_key)
            
            # Apply repair if needed
            repaired_pcm = repair_if_early_terminated(
                raw,
                original_text=text,
                tts_base=TTS_BASE,
                sample_rate=SAMPLE_RATE,
                token=TTS_TOKEN,
                keep_original=False
            )
            
            # Ensure mu-law format for telephony
            final_raw = transcode_to_mulaw_8k_mono(repaired_pcm, sr_out=8000)
            
            # Upload to final location
            put_s3_bytes(bucket, dst_key, final_raw, content_type="audio/wav")
            
            # Clean up source file
            try:
                s3.delete_object(Bucket=bucket, Key=src_key)
            except Exception as e:
                print(f"[WARN] Could not delete {src_key}: {e}")
            
            final_url = to_regional_url(bucket, AWS_REGION, dst_key)
            print(f"    -> s3://{bucket}/{dst_key}")
            
            neutral_rows.append({
                "category": f"neutral_{category}",
                "index": idx,
                "text": text,
                "bucket": bucket,
                "final_key": dst_key,
                "final_url": final_url,
                "usage": "fallback_response"
            })
            
            # Add to UtteranceCache for potential reuse
            norm = normalize_utt(text)
            h = utt_hash(norm)
            utt_tbl.put_item(Item={
                "utterance_hash": h,
                "locale": "ko-KR",
                "normalized_utterance": norm,
                "audio_s3_uri": f"s3://{bucket}/{dst_key}",
                "status": "approved",
                "approved_by": "preset_loader",
                "created_at": int(time.time()),
                "num_hits": 0,
                "notes": f"neutral:{category} #{idx}"
            })
    
    return neutral_rows

def get_default_neutral_message():
    """Get the primary neutral message for cache misses"""
    # This should be the most versatile message that works for all scenarios
    return "ê³ ê°ë‹˜ì˜ ë§ì”€ ìž˜ ë“¤ì—ˆìŠµë‹ˆë‹¤. ê²€í†  í›„ ë‹¤ì‹œ ì—°ë½ë“œë¦¬ê² ìŠµë‹ˆë‹¤."

def get_contextual_neutral_message(customer_response_type="general"):
    """
    Get appropriate neutral message based on customer response context
    customer_response_type: "general", "busy_response", "consideration", "not_interested"
    """
    if customer_response_type in NEUTRAL_MESSAGES:
        # Return first message from the category (you could randomize this)
        return NEUTRAL_MESSAGES[customer_response_type][0]
    return get_default_neutral_message()


# ---------- Helpers ----------
import io, re, numpy as np, soundfile as sf

def voiced_duration_sec(wav_bytes: bytes, sr_expected=8000, frame_ms=10, thr=0.02) -> float:
    """Approximate voiced duration using a simple RMS threshold."""
    audio, sr = sf.read(io.BytesIO(wav_bytes), dtype="float32", always_2d=False)
    if audio.ndim > 1:
        audio = audio.mean(axis=1)
    # (sr mismatch is rare here; good enough for detection)
    frame = max(1, int(sr * (frame_ms / 1000.0)))
    last_voiced = 0
    for i in range(0, len(audio) - frame, frame):
        rms = float(np.sqrt(np.mean(np.square(audio[i:i+frame])) + 1e-9))
        if rms > thr:
            last_voiced = i + frame
    return last_voiced / float(sr if sr else sr_expected)

_SENT_END_RE = re.compile(r"(?<=[\.!\?â€¦])\s+")

def split_korean_sentences(text: str) -> list[str]:
    parts = _SENT_END_RE.split(text.strip())
    return [p for p in parts if p]

def _should_stop(now_tokens, saw_eos, ms_since_eos, ms_silence, have_audio_ms):
    # 1) Don't allow EOS too early
    if saw_eos and now_tokens < MIN_TOKENS_BEFORE_EOS:
        return False
    # 2) After EOS, insist on a grace window of extra audio
    if saw_eos and ms_since_eos < EOS_GRACE_MS:
        return False
    # 3) Only stop on silence if it's held long enough *and* we've generated enough speech
    if ms_silence >= SILENCE_HANGOVER_MS and have_audio_ms >= 1000:
        return True
    # 4) If EOS was seen long enough ago, it's okay to stop
    if saw_eos and ms_since_eos >= EOS_GRACE_MS:
        return True
    return False

def audio_rms(x):
    import numpy as np
    if x is None or len(x) == 0: return 0.0
    return float(np.sqrt((x.astype("float32") ** 2).mean() + 1e-9))

_ZWJ = "\u2060"  # WORD JOINER (prevents 'ê² ' dropout)

def prepare_for_tts(text: str) -> str:
    """
    TTS-only preprocessing (do NOT use for hashing):
      - NFKC normalize & trim
      - Strip simple markdown markers
      - Make URLs speakable in Korean
      - Expand % -> 'í¼ì„¼íŠ¸'
      - Prevent 'ê² ' skip with WORD JOINER
      - Gentle sentence-final tail
    """
    s = unicodedata.normalize("NFKC", text).strip()

    # 1) Strip simple markdown
    s = s.replace("**", "").replace("__", "").replace("*", "").replace("_", "").replace("`", "")

    # 2) URLs -> speakable
    def _speak_url(m):
        full = m.group(0)
        core = re.sub(r"^https?://", "", full, flags=re.I)
        core = re.sub(r"^www\.", "ë”ë¸”ìœ  ë”ë¸”ìœ  ë”ë¸”ìœ  ì  ", core, flags=re.I)
        core = core.replace(".com", " ë‹·ì»´")
        core = core.replace(".kr", " ì  ì¼€ì´ì•Œ")
        core = core.replace(".", " ì  ")
        core = core.replace("/", " ìŠ¬ëž˜ì‹œ ")
        return core.strip()
    s = re.sub(r"https?://\S+|www\.\S+", _speak_url, s, flags=re.I)

    # 3) Safe glyph & numeric expansions
    s = s.replace("ì¼‡", "ì¼œ")                 # rare corruption guard
    s = re.sub(r"(\d+)\s*%", r"\1 í¼ì„¼íŠ¸", s)  # 15% -> 15 í¼ì„¼íŠ¸

    s = re.sub(
        r"[\U0001F600-\U0001F64F"
        r"\U0001F300-\U0001F5FF"
        r"\U0001F680-\U0001F6FF"
        r"\U0001F1E0-\U0001F1FF"
        r"\u2600-\u26FF\u2700-\u27BF]+",
        "",
        s
    )
    s = re.sub(r"[:;][\-\^]?[)D(]+", "", s)

    # 4) Collapse whitespace
    s = re.sub(r"\s+", " ", s)

    # 5) Prevent 'ê² ' dropout before common endings (TTS-only)
    s = re.sub(r"ê² (?=(ë‹¤|ìŠµë‹ˆë‹¤|ì–´ìš”|ì—ìš”|ì§€ìš”|ë„¤ìš”|êµ°ìš”|ê³ ìš”|ì£ ))", "ê² " + _ZWJ, s)

    # 6) Ensure a gentle sentence ending (so engines don't truncate)
    if not re.search(r"[.!?â€¦~ë‹¤ìš”]$", s):
        s += "â€¦"

    return s

def stitch_float_wavs(wavs: list[bytes]) -> bytes:
    """Concatenate several WAV byte blobs (PCM16/float) into one PCM16 WAV, with a tiny gap."""
    arrs = []
    sr_ref = None
    for wb in wavs:
        a, sr = sf.read(io.BytesIO(wb), dtype="float32", always_2d=False)
        if a.ndim > 1:
            a = a.mean(axis=1)
        if sr_ref is None:
            sr_ref = sr
        arrs.append(a)
        # 40 ms gap between sentences to avoid clicks / rushed joins
        gap = np.zeros(int((sr_ref or 8000) * 0.04), dtype="float32")
        arrs.append(gap)
    if arrs:
        arrs = arrs[:-1]  # drop last gap
    cat = np.concatenate(arrs) if arrs else np.zeros(0, dtype="float32")
    buf = io.BytesIO()
    sf.write(buf, cat, sr_ref or 8000, subtype="PCM_16", format="WAV")
    return buf.getvalue()

def repair_if_early_terminated(raw_wav: bytes, original_text: str, tts_base: str, sample_rate: int, token: str, keep_original=False) -> bytes:
    """
    If speech ends too early (or likely risky ending like 'ê² ìŠµë‹ˆë‹¤'), re-synthesize by sentence with short TTS and stitch.
    Returns repaired PCM WAV bytes (or the input if no repair needed).
    """
    v_sec = voiced_duration_sec(raw_wav, sr_expected=sample_rate)
    # total duration from header
    audio_len, sr = sf.read(io.BytesIO(raw_wav), dtype="float32", always_2d=False)
    total_sec = len(audio_len) / float(sr if sr else sample_rate)

    risky = ("ê² ìŠµë‹ˆë‹¤" in original_text)  # common KR polite tail that sometimes drops
    needs_repair = (v_sec < 0.7 * total_sec) or (risky and v_sec < total_sec - 0.3)
    if not needs_repair:
        return raw_wav  # looks fine

    print(f"[INFO] Early termination detected (voiced={v_sec:.2f}s/{total_sec:.2f}s). Repairing by sentencesâ€¦")
    sents = split_korean_sentences(original_text)
    if not sents:
        sents = [original_text]

    parts = []
    for s in sents:
        tts_s = prepare_for_tts(s)
        info_seg = synthesize_short(tts_base, tts_s, key_prefix="repair", sr=sample_rate, token=token)
        b2, k2 = info_seg.get("bucket"), info_seg.get("key")
        if not b2 or not k2:
            continue
        seg_bytes = fetch_s3_bytes(b2, k2)
        parts.append(seg_bytes)
        if not keep_original:
            try:
                s3.delete_object(Bucket=b2, Key=k2)
            except Exception:
                pass
    if not parts:
        return raw_wav  # fallback: keep original

    return stitch_float_wavs(parts)


_ZWJ = "\u2060"  # WORD JOINER (prevents 'ê² ' dropout)

def prepare_for_tts(text: str) -> str:
    """
    TTS-only preprocessing (do NOT use for hashing):
      - NFKC normalize & trim
      - Strip simple markdown markers
      - Make URLs speakable in Korean
      - Expand % -> 'í¼ì„¼íŠ¸'
      - Prevent 'ê² ' skip with WORD JOINER
      - Gentle sentence-final tail
    """
    s = unicodedata.normalize("NFKC", text).strip()

    # 1) Strip simple markdown
    s = s.replace("**", "").replace("__", "").replace("*", "").replace("_", "").replace("`", "")

    # 2) URLs -> speakable
    def _speak_url(m):
        full = m.group(0)
        core = re.sub(r"^https?://", "", full, flags=re.I)
        core = re.sub(r"^www\.", "ë”ë¸”ìœ  ë”ë¸”ìœ  ë”ë¸”ìœ  ì  ", core, flags=re.I)
        core = core.replace(".com", " ë‹·ì»´")
        core = core.replace(".kr", " ì  ì¼€ì´ì•Œ")
        core = core.replace(".", " ì  ")
        core = core.replace("/", " ìŠ¬ëž˜ì‹œ ")
        return core.strip()
    s = re.sub(r"https?://\S+|www\.\S+", _speak_url, s, flags=re.I)

    # 3) Safe glyph & numeric expansions
    s = s.replace("ì¼‡", "ì¼œ")                 # rare corruption guard
    s = re.sub(r"(\d+)\s*%", r"\1 í¼ì„¼íŠ¸", s)  # 15% -> 15 í¼ì„¼íŠ¸

    s = re.sub(
        r"[\U0001F600-\U0001F64F"
        r"\U0001F300-\U0001F5FF"
        r"\U0001F680-\U0001F6FF"
        r"\U0001F1E0-\U0001F1FF"
        r"\u2600-\u26FF\u2700-\u27BF]+",
        "",
        s
    )
    s = re.sub(r"[:;][\-\^]?[)D(]+", "", s)

    # 4) Collapse whitespace
    s = re.sub(r"\s+", " ", s)

    # 5) Prevent 'ê² ' dropout before common endings (TTS-only)
    s = re.sub(r"ê² (?=(ë‹¤|ìŠµë‹ˆë‹¤|ì–´ìš”|ì—ìš”|ì§€ìš”|ë„¤ìš”|êµ°ìš”|ê³ ìš”|ì£ ))", "ê² " + _ZWJ, s)

    # 6) Ensure a gentle sentence ending (so engines don't truncate)
    if not re.search(r"[.!?â€¦~ë‹¤ìš”]$", s):
        s += "â€¦"

    return s

def normalize_utt(text: str) -> str:
    return text.strip().lower()

def utt_hash(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()

def has_ffmpeg() -> bool:
    return shutil.which("ffmpeg") is not None

def to_regional_url(bucket: str, region: str, key: str) -> str:
    return f"https://{bucket}.s3.{region}.amazonaws.com/{key}"

def parse_wav_fmt_sr_ch(wav_bytes: bytes):
    bio = io.BytesIO(wav_bytes)
    if bio.read(4) != b'RIFF': return (None, None, None)
    _ = bio.read(4)
    if bio.read(4) != b'WAVE': return (None, None, None)
    while True:
        hdr = bio.read(8)
        if len(hdr) < 8: return (None, None, None)
        chunk_id, chunk_sz = hdr[:4], struct.unpack("<I", hdr[4:8])[0]
        if chunk_id == b'fmt ':
            fmt_data = bio.read(chunk_sz)
            if len(fmt_data) < 16: return (None, None, None)
            fmt_code   = struct.unpack("<H", fmt_data[0:2])[0]
            channels   = struct.unpack("<H", fmt_data[2:4])[0]
            sample_rate= struct.unpack("<I", fmt_data[4:8])[0]
            return (fmt_code, channels, sample_rate)
        else:
            bio.seek(chunk_sz + (chunk_sz % 2), io.SEEK_CUR)

def is_mulaw_8k_mono(wav_bytes: bytes) -> bool:
    fmt_code, ch, sr = parse_wav_fmt_sr_ch(wav_bytes)
    return (fmt_code == 7 and ch == 1 and sr == 8000)

def transcode_to_mulaw_8k_mono(in_wav_bytes: bytes, sr_out: int = 8000) -> bytes:
    if not has_ffmpeg():
        raise RuntimeError("ffmpeg not found in PATH; please install it.")
    # Add ~0.5s of silence tail at transcode time
    cmd = [
        "ffmpeg", "-hide_banner", "-loglevel", "error",
        "-f", "wav", "-i", "pipe:0",
        "-ar", str(sr_out), "-ac", "1", "-c:a", "pcm_mulaw",
        "-af", "apad=pad_dur=0.5",          # â† tail padding at encode
        "-f", "wav", "pipe:1"
    ]
    proc = subprocess.run(cmd, input=in_wav_bytes, capture_output=True, check=True)
    return proc.stdout


def fetch_s3_bytes(bucket: str, key: str) -> bytes:
    obj = s3.get_object(Bucket=bucket, Key=key)
    return obj["Body"].read()

def put_s3_bytes(bucket: str, key: str, data: bytes, content_type: str = "audio/wav"):
    s3.put_object(Bucket=bucket, Key=key, Body=data, ContentType=content_type)

# ---------- TTS clients ----------
def _headers(token: str = ""):
    h = {"Content-Type": "application/json"}
    if token: h["Authorization"] = f"Bearer {token}"
    return h

def generate_neutral_by_temps(temps, base_category="test", message=None,
                              top_p=None, repetition_penalty=None,
                              max_new_tokens=None, chunk_length=None,
                              use_memory_cache=None):
    """
    For each temperature in `temps`, synthesize the neutral message once and store as:
    s3://<bucket>/neutral/<base_category>/T<temp>/<idx>.wav
    Returns a list of metadata rows, just like your other generators.
    """
    rows = []
    messages = [message] if message else NEUTRAL_MESSAGES.get(base_category, [])
    if not messages:
        print(f"[WARN] No messages found for category '{base_category}'")
        return rows

    for temp in temps:
        temp_tag = f"T{float(temp):.2f}".replace(".", "_")  # e.g., T0_80
        for idx, text in enumerate(messages, start=1):
            tts_text = prepare_for_tts(text)

            info = synthesize_short(
                TTS_BASE,
                tts_text,
                key_prefix=f"neutral/{base_category}/{temp_tag}",
                sr=SAMPLE_RATE,
                token=TTS_TOKEN,
                temperature=float(temp),
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                max_new_tokens=max_new_tokens,
                chunk_length=chunk_length,
                use_memory_cache=use_memory_cache,
            )

            bucket = info.get("bucket"); src_key = info.get("key")
            if not bucket or not src_key:
                print(f"[ERROR] Failed at temp={temp} #{idx}")
                continue

            # fetch, repair (if early stop), transcode to Âµ-law/8k/mono
            raw = fetch_s3_bytes(bucket, src_key)
            repaired_pcm = repair_if_early_terminated(
                raw, original_text=text, tts_base=TTS_BASE, sample_rate=SAMPLE_RATE,
                token=TTS_TOKEN, keep_original=False
            )
            final_raw = transcode_to_mulaw_8k_mono(repaired_pcm, sr_out=8000)

            dst_key = f"neutral/{base_category}/{temp_tag}/{idx:02d}.wav"
            put_s3_bytes(bucket, dst_key, final_raw, content_type="audio/wav")
            try:
                s3.delete_object(Bucket=bucket, Key=src_key)
            except Exception:
                pass

            final_url = to_regional_url(bucket, AWS_REGION, dst_key)
            print(f"[OK] temp={temp:.2f} â†’ s3://{bucket}/{dst_key}")

            rows.append({
                "category": f"neutral_{base_category}_{temp_tag}",
                "index": idx,
                "text": text,
                "bucket": bucket,
                "final_key": dst_key,
                "final_url": final_url,
                "temperature": float(temp),
                "usage": "fallback_response"
            })
    return rows


def synthesize_short(tts_base: str, text: str, key_prefix: str, sr: int, token: str = "",
                     temperature: float | None = None, top_p: float | None = None,
                     repetition_penalty: float | None = None, max_new_tokens: int | None = None,
                     chunk_length: int | None = None, use_memory_cache: bool | None = None):
    url = f"{tts_base}/synthesize"
    payload = {
        "text": text,
        "sample_rate": sr,
        "key_prefix": key_prefix,
    }
    # only include provided knobs
    if temperature is not None:        payload["temperature"] = float(temperature)
    if top_p is not None:              payload["top_p"] = float(top_p)
    if repetition_penalty is not None: payload["repetition_penalty"] = float(repetition_penalty)
    if max_new_tokens is not None:     payload["max_new_tokens"] = int(max_new_tokens)
    if chunk_length is not None:       payload["chunk_length"] = int(chunk_length)
    if use_memory_cache is not None:   payload["use_memory_cache"] = bool(use_memory_cache)

    last_err = None
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            resp = requests.post(url, json=payload, headers=_headers(token), timeout=60)
            if resp.status_code == 200:
                return resp.json()
            last_err = f"HTTP {resp.status_code}: {resp.text[:300]}"
        except Exception as e:
            last_err = str(e)
        time.sleep(RETRY_SLEEP_SEC * attempt)
    raise RuntimeError(f"/synthesize failed: {last_err}")

def synthesize_long_stream(tts_base: str, text: str, sr: int, token: str = "") -> tuple[str, str]:
    """
    Start streaming job and return (job_id, bucket). Youâ€™ll poll parts and stitch.
    """
    url = f"{tts_base}/synthesize_stream_start"
    payload = {"text": text, "sample_rate": sr}
    resp = requests.post(url, json=payload, headers=_headers(token), timeout=30)
    resp.raise_for_status()
    data = resp.json()
    return data["job_id"], data["bucket"]

def poll_stream_batch(tts_base: str, job_id: str, start_idx: int = 0, limit: int = 8, token: str = "", expires: int = 600):
    url = f"{tts_base}/synthesize_stream_batch"
    params = {"job_id": job_id, "start_idx": start_idx, "limit": limit, "expires": expires}
    resp = requests.get(url, params=params, headers=_headers(token), timeout=30)
    resp.raise_for_status()
    return resp.json()

def download_wav(url: str) -> bytes:
    r = requests.get(url, timeout=60)
    r.raise_for_status()
    return r.content

def stitch_wavs_to_bytes(urls: list[str]) -> bytes:
    """Download each WAV URL, read as float32 mono, concat, return WAV bytes (float PCM).
       Weâ€™ll Âµ-law-transcode later anyway."""
    floats = []
    sr_ref = None
    for u in urls:
        audio, sr = sf.read(io.BytesIO(download_wav(u)), dtype="float32", always_2d=False)
        if audio.ndim > 1:
            audio = audio.mean(axis=1)
        if sr_ref is None:
            sr_ref = sr
        elif sr != sr_ref:
            # resample lightly via soundfile+numpy if needed (rare for your server)
            import numpy as np
            t = np.linspace(0.0, 1.0, num=audio.size, endpoint=False, dtype=np.float32)
            # (simple resample omitted for brevity; your server outputs consistent SR)
        floats.append(audio)
    import numpy as np
    cat = np.concatenate(floats) if floats else np.zeros(0, dtype="float32")
    buf = io.BytesIO()
    sf.write(buf, cat, sr_ref or 8000, subtype="PCM_16", format="WAV")
    return buf.getvalue()

# ---------- Chatbot ----------
def get_chatbot_answer(utt: str) -> str:
    payload = {"session_id": "preset-loader", "question": utt}
    r = requests.post(CHATBOT_URL, data=json.dumps(payload), headers={"Content-Type": "application/json"}, timeout=30)
    r.raise_for_status()
    try:
        data = r.json()
    except Exception:
        return r.text
    return data.get("answer") or data.get("response") or str(data)

# ---------- Main ----------
def main():
    parser = argparse.ArgumentParser(description="Hybrid synth presets (short fillers via /synthesize; FAQ via streaming).")
    parser.add_argument("--tts-url", default=DEFAULT_TTS_URL, help="app.py URL (e.g., http://host:8000/synthesize)")
    parser.add_argument("--sample-rate", type=int, default=SAMPLE_RATE, help="target sample rate (default 8000)")
    parser.add_argument("--token", default=TTS_TOKEN, help="Bearer token if server requires auth")
    parser.add_argument("--region", default=AWS_REGION, help="AWS region for S3")
    parser.add_argument("--keep-original", action="store_true", help="Do not delete UUID source objects created server-side")
    parser.add_argument("--out-csv", default="filler_index.csv", help="Index CSV path")
    parser.add_argument("--force-transcode", action="store_true", help="Always transcode to Âµ-law even if already correct")
    parser.add_argument("--neutral-only", default=True, action="store_true", help="Generate only neutral voices")
    parser.add_argument("--sweep", action="store_true", help="run temperature sweep for test category")
    # change default temps to empty
    parser.add_argument("--temps", default="0.9", help="Comma-separated temperatures (used only with --sweep)")
    args = parser.parse_args()

    rows = []
    total = sum(len(v) for v in CATEGORIES.values())
    i = 0

    if not has_ffmpeg():
        print("[WARN] ffmpeg not found; non-Âµlaw inputs will be force-converted later.")
    
    # Generate neutral voices
    print("=== Generating Neutral Voices ===")
    neutral_rows = generate_neutral_voices()
    rows.extend(neutral_rows)

    # Also generate the neutral 'test' message across different temperatures for A/B comparison
    temps = [float(x) for x in (args.temps.split(",") if args.temps else []) if x.strip()]
    if args.sweep and temps:
        print(f"=== Generating Neutral 'test' across temperatures: {temps} ===")
        sweep_rows = generate_neutral_by_temps(
            temps,
            base_category="promotional",                                # <- use your real lineâ€™s bucket path
            message=NEUTRAL_MESSAGES["promotional"][0],                 # <- generate exactly THIS one
            top_p=0.8, repetition_penalty=1.25, max_new_tokens=128,
            chunk_length=128, use_memory_cache=False,
        )
        rows.extend(sweep_rows)
    
    if not args.neutral_only:

        # -------- Fillers (SHORT) --------
        print(f"Starting fillers â†’ {TTS_BASE}/synthesize")
        for category, phrases in CATEGORIES.items():
            for idx, text in enumerate(phrases, start=1):
                i += 1
                print(f"[{i}/{total}] {category} #{idx}: \"{text}\" (short)")
                orig = text
                norm = normalize_utt(orig)         # key
                utt_h = utt_hash(norm)

                tts_text = prepare_for_tts(orig)
                info = synthesize_short(TTS_BASE, tts_text, key_prefix=category, sr=args.sample_rate, token=args.token)  # single-shot:contentReference[oaicite:3]{index=3}
                bucket = info.get("bucket"); src_key = info.get("key")
                if not bucket or not src_key:
                    raise RuntimeError(f"Server did not return bucket/key for {category} #{idx}: {info}")

                dst_key = f"{category}/{idx:02d}.wav"
                raw = fetch_s3_bytes(bucket, src_key)

                # ADD this block ðŸ‘‡
                repaired_pcm = repair_if_early_terminated(
                    raw,
                    original_text=orig,          # <- the original text (no tail mark)
                    tts_base=TTS_BASE,
                    sample_rate=args.sample_rate,
                    token=args.token,
                    keep_original=bool(args.keep_original),
                )

                must_transcode = args.force_transcode or (not is_mulaw_8k_mono(raw))
                if must_transcode:
                    raw = transcode_to_mulaw_8k_mono(raw, sr_out=8000)
                    print("    -> transcoded to Âµ-law/8k/mono")

                put_s3_bytes(bucket, dst_key, raw, content_type="audio/wav")

                if not args.keep_original:
                    try: s3.delete_object(Bucket=bucket, Key=src_key)
                    except ClientError as e: print(f"[WARN] Could not delete original {src_key}: {e}")

                final_url = to_regional_url(bucket, args.region, dst_key)
                print(f"    -> s3://{bucket}/{dst_key}")
                print(f"    -> URL: {final_url}")

                rows.append({
                    "category": category, "index": idx, "text": text,
                    "bucket": bucket, "final_key": dst_key, "final_url": final_url,
                    "src_key": src_key, "server_url": info.get("url") or info.get("s3_url"),
                    "sample_rate": info.get("sample_rate", args.sample_rate),
                    "transcoded": "yes" if must_transcode else "no",
                })

                # â€” DDB: utterance-only row (preset)
                norm = normalize_utt(text); h = utt_hash(norm)
                utt_tbl.put_item(Item={
                    "utterance_hash": h,
                    "locale": "ko-KR",
                    "normalized_utterance": norm,
                    "audio_s3_uri": f"s3://{bucket}/{dst_key}",
                    "status": "approved",
                    "approved_by": "preset_loader",
                    "created_at": int(time.time()),
                    "num_hits": 0,
                    "notes": f"preset:{category} #{idx}"
                })


        # -------- FAQ via Chatbot (LONG streaming path) --------
        print("\n--- Preloading FAQ answers via chatbot (streaming) ---")
        for utt in FAQS:
            print(f"FAQ: Q=\"{utt}\"  â†’ asking chatbotâ€¦")
            answer_text = get_chatbot_answer(utt)  # your chatbotâ€™s answer text
            answer_text = prepare_for_tts(answer_text)
            norm_utt  = normalize_utt(utt);   utt_h  = utt_hash(norm_utt)
            norm_resp = normalize_utt(answer_text); resp_h = utt_hash(norm_resp)

            # Start stream job
            job_id, bucket = synthesize_long_stream(TTS_BASE, answer_text, sr=args.sample_rate, token=args.token)  # streaming API:contentReference[oaicite:4]{index=4}

            # Poll parts until no more (simple bounded loop)
            part_idx = 0
            collected_urls = []
            max_empty_polls = 20
            empty_polls = 0
            while True:
                batch = poll_stream_batch(TTS_BASE, job_id, start_idx=part_idx, limit=6, token=args.token, expires=600)
                urls = batch.get("AudioS3Urls", []) or []
                if not urls:
                    empty_polls += 1
                    if empty_polls >= max_empty_polls:
                        break
                    time.sleep(0.3)
                    continue
                empty_polls = 0
                collected_urls.extend(urls)
                part_idx = int(batch.get("NextIndexOut", part_idx))
                has_more = str(batch.get("HasMore", "")).lower() == "true"
                if not has_more:
                    break

            # After polling collected_urls
            if not collected_urls:
                # 1) Try final.wav from the job path
                final_key_guess = f"{job_id}/final.wav"
                try:
                    s3.head_object(Bucket=bucket, Key=final_key_guess)
                    # If exists, download and use it
                    final_bytes = fetch_s3_bytes(bucket, final_key_guess)
                    final_raw = transcode_to_mulaw_8k_mono(final_bytes, sr_out=8000)
                    final_key = f"faq/{utt_h}.wav"
                    put_s3_bytes(bucket, final_key, final_raw, content_type="audio/wav")
                    print(f"    -> (fallback final.wav) s3://{bucket}/{final_key}")
                except Exception:
                    print(f"[WARN] No parts & no final.wav; falling back to /synthesize for: {utt}")
                    info2 = synthesize_short(TTS_BASE, answer_text, key_prefix="faq", sr=args.sample_rate, token=args.token)
                    bucket2 = info2.get("bucket")
                    src_key2 = info2.get("key")
                    if not src_key2 or not bucket2:
                        print(f"[ERROR] Short fallback failed for: {utt}")
                        continue

                    raw2 = fetch_s3_bytes(bucket2, src_key2)
                    repaired_pcm = repair_if_early_terminated(
                        raw2,
                        original_text=answer_text,
                        tts_base=TTS_BASE,
                        sample_rate=args.sample_rate,
                        token=args.token,
                        keep_original=bool(args.keep_original),
                    )

                    final_raw = transcode_to_mulaw_8k_mono(repaired_pcm, sr_out=8000)
                    final_key = f"faq/{utt_h}.wav"
                    put_s3_bytes(bucket2, final_key, final_raw, content_type="audio/wav")
                    if not args.keep_original:
                        try:
                            s3.delete_object(Bucket=bucket2, Key=src_key2)
                        except Exception as e:
                            print(f"[WARN] Could not delete original {src_key2}: {e}")


                # Write DDB row (utterance + response hashes)
                utt_tbl.put_item(Item={
                    "utterance_hash": utt_h,
                    "locale": "ko-KR",
                    "normalized_utterance": norm_utt,
                    "response_hash": resp_h,
                    "normalized_response": norm_resp,
                    "audio_s3_uri": f"s3://{bucket}/{final_key}",
                    "status": "approved",
                    "approved_by": "preset_loader",
                    "created_at": int(time.time()),
                    "num_hits": 0,
                    "notes": "faq bootstrap (fallback)"
                })
                continue  # go to next FAQ

            # Stitch parts â†’ WAV bytes
            stitched_wav = stitch_wavs_to_bytes(collected_urls)
            # Transcode to Î¼-law/8k/mono (safety)
            repaired_pcm = repair_if_early_terminated(
                stitched_wav,
                original_text=answer_text,   # the chatbot answer BEFORE transcode
                tts_base=TTS_BASE,
                sample_rate=args.sample_rate,
                token=args.token,
                keep_original=bool(args.keep_original),
            )

            final_raw = transcode_to_mulaw_8k_mono(repaired_pcm, sr_out=8000)

            final_key = f"faq/{utt_h}.wav"
            put_s3_bytes(bucket, final_key, final_raw, content_type="audio/wav")
            print(f"    -> s3://{bucket}/{final_key}")

            # DDB row with both utterance + response hashes
            utt_tbl.put_item(Item={
                "utterance_hash": utt_h,
                "locale": "ko-KR",
                "normalized_utterance": norm_utt,
                "response_hash": resp_h,
                "normalized_response": norm_resp,
                "audio_s3_uri": f"s3://{bucket}/{final_key}",
                "status": "approved",
                "approved_by": "preset_loader",
                "created_at": int(time.time()),
                "num_hits": 0,
                "notes": "faq bootstrap"
            })

    # -------- Write index CSV --------
    if rows:
        with open("filler_index.csv", "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=list(rows[0].keys()), extrasaction="ignore")
            w.writeheader(); w.writerows(rows)
        print("\nDone. Wrote index CSV: filler_index.csv")
    else:
        print("No filler rows to write.")

if __name__ == "__main__":
    main()